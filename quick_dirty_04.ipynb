{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dbd40f12-3101-4b9c-ad24-cf2a34590971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO,BytesIO# bytesio because we are saving in s3 in \n",
    "#parquet format whichnwill be usefull in orchastrationa nd airflow\n",
    "from datetime import datetime,timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4a3e17-6788-4c70-82df-4f3bf7a28972",
   "metadata": {},
   "source": [
    "# Adapter Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6b285c30-703d-4422-98f1-dc2975e11cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter Layer\n",
    "\n",
    "def read_csv_to_df(bucket, key, decoding = 'utf-8', sep = ','):\n",
    "    csv_obj = bucket.Object(key=key).get().get('Body').read().decode(decoding)\n",
    "    data = StringIO(csv_obj)\n",
    "    df = pd.read_csv(data, delimiter=sep)\n",
    "    return df\n",
    "\n",
    "def write_df_to_s3(bucket, df, key):\n",
    "    out_buffer = BytesIO()\n",
    "    df.to_parquet(out_buffer, index=False)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "def write_df_to_s3_csv(bucket, df, key):\n",
    "    out_buffer = StringIO()\n",
    "    df.to_csv(out_buffer, index=False)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "def list_files_in_prefix(bucket, prefix):\n",
    "    files = [obj.key for obj in bucket.objects.filter(Prefix=prefix)]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40201c4a-9d44-45ca-a844-fd44b7b35095",
   "metadata": {},
   "source": [
    "# Application layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "88890dbe-80f7-435e-a08f-bd523255d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application Layer\n",
    "\n",
    "def extract(bucket, date_list):\n",
    "    files = [key for date in date_list for key in list_files_in_prefix(bucket, date)]\n",
    "    df = pd.concat([read_csv_to_df(bucket, obj) for obj in files], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def transform_report1(df, columns, arg_date):\n",
    "    df = df.loc[:, columns]\n",
    "    df.dropna(inplace=True)\n",
    "    df['opening_price'] = df.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('first')\n",
    "    df['closing_price'] = df.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('last')\n",
    "    df = df.groupby(['ISIN', 'Date'], as_index=False).agg(opening_price_eur=('opening_price', 'min'), closing_price_eur=('closing_price', 'min'), minimum_price_eur=('MinPrice', 'min'), maximum_price_eur=('MaxPrice', 'max'), daily_traded_volume=('TradedVolume', 'sum'))\n",
    "    df['prev_closing_price'] = df.sort_values(by=['Date']).groupby(['ISIN'])['closing_price_eur'].shift(1)\n",
    "    df['change_prev_closing_%'] = (df['closing_price_eur'] - df['prev_closing_price']) / df['prev_closing_price'] * 100\n",
    "    df.drop(columns=['prev_closing_price'], inplace=True)\n",
    "    df = df.round(decimals=2)\n",
    "    df = df[df.Date >= arg_date]\n",
    "    return df\n",
    "\n",
    "def load(bucket, df, trg_key, trg_format,meta_key, extract_date_list):\n",
    "    key = trg_key + datetime.today().strftime(\"%Y%m%d_%H%M%S\") + trg_format\n",
    "    write_df_to_s3(bucket, df, key)\n",
    "    update_meta_file(bucket, meta_key, extract_date_list)\n",
    "    return True\n",
    "\n",
    "def etl_report1(src_bucket, trg_bucket, date_list, columns, arg_date, trg_key, trg_format,meta_key):\n",
    "    df = extract(src_bucket, date_list)\n",
    "    df = transform_report1(df, columns, arg_date)\n",
    "    extract_date_list = [date for date in date_list if date >= arg_date]\n",
    "    load(trg_bucket, df, trg_key, trg_format,meta_key, extract_date_list)\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0dc4bfc5-7091-4257-87d2-af5176bffd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_date_list(bucket, arg_date, src_format, meta_key):\n",
    "    min_date = datetime.strptime(arg_date, src_format).date() - timedelta(days=1)\n",
    "    today = datetime.today().date()\n",
    "    try:\n",
    "        df_meta = read_csv_to_df(bucket, meta_key)\n",
    "        dates = [(min_date + timedelta(days=x)) for x in range(0, (today-min_date).days + 1)]\n",
    "        src_dates = set(pd.to_datetime(df_meta['source_date']).dt.date)\n",
    "        \n",
    "        # Include the date immediately after the last available date in df_meta\n",
    "        if src_dates:\n",
    "            last_date_in_meta = max(src_dates)\n",
    "            min_date = last_date_in_meta\n",
    "        else:\n",
    "            min_date = min_date\n",
    "\n",
    "        dates_missing = set(dates) - src_dates\n",
    "\n",
    "        if dates_missing:\n",
    "            min_date = min(dates_missing)\n",
    "            return_dates = [date.strftime(src_format) for date in dates if date >= min_date]\n",
    "            return_min_date = min_date.strftime(src_format)\n",
    "        else:\n",
    "            return_dates = []\n",
    "            return_min_date = datetime(2200, 1, 1).strftime(src_format)\n",
    "    except bucket.meta.client.exceptions.NoSuchKey:\n",
    "        return_dates = [(min_date + timedelta(days=x)).strftime(src_format) for x in range(0, (today-min_date).days + 1)]\n",
    "        return_min_date = arg_date\n",
    "    return return_min_date, return_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e1e5a587-bac0-492d-9fec-7e14ce1a23e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Application Layer - not core\\n\\ndef return_date_list(bucket, arg_date, src_format, meta_key):\\n    min_date = datetime.strptime(arg_date, src_format).date() - timedelta(days=1)\\n    today = datetime.today().date()\\n    try:\\n        df_meta = read_csv_to_df(bucket, meta_key)\\n        dates = [(min_date + timedelta(days=x)) for x in range(0, (today-min_date).days + 1)]\\n        src_dates = set(pd.to_datetime(df_meta['source_date']).dt.date)\\n        dates_missing = set(dates[1:]) - src_dates\\n        if dates_missing:\\n            min_date = min(set(dates[1:]) - src_dates) - timedelta(days=1)\\n            return_dates = [date.strftime(src_format) for date in dates if date >= min_date]\\n            return_min_date = (min_date + timedelta(days=1)).strftime(src_format)\\n        else:\\n            return_dates = []\\n            return_min_date = datetime(2200, 1, 1).date()\\n    except bucket.meta.client.exceptions.NoSuchKey:\\n        return_dates = [(min_date + timedelta(days=x)).strftime(src_format) for x in range(0, (today-min_date).days + 1)]\\n        return_min_date = arg_date\\n    return return_min_date, return_dates\\n\\ndef update_meta_file(bucket, meta_key, extract_date_list):\\n    df_new = pd.DataFrame(columns=['source_date', 'datetime_of_processing'])\\n    df_new['source_date'] = extract_date_list\\n    df_new['datetime_of_processing'] = datetime.today().strftime('%Y-%m-%d')\\n    df_old = read_csv_to_df(bucket, meta_key)\\n    df_all = pd.concat([df_old, df_new])\\n    write_df_to_s3_csv(bucket, df_all, meta_key)\""
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Application Layer - not core\n",
    "\n",
    "def return_date_list(bucket, arg_date, src_format, meta_key):\n",
    "    min_date = datetime.strptime(arg_date, src_format).date() - timedelta(days=1)\n",
    "    today = datetime.today().date()\n",
    "    try:\n",
    "        df_meta = read_csv_to_df(bucket, meta_key)\n",
    "        dates = [(min_date + timedelta(days=x)) for x in range(0, (today-min_date).days + 1)]\n",
    "        src_dates = set(pd.to_datetime(df_meta['source_date']).dt.date)\n",
    "        dates_missing = set(dates[1:]) - src_dates\n",
    "        if dates_missing:\n",
    "            min_date = min(set(dates[1:]) - src_dates) - timedelta(days=1)\n",
    "            return_dates = [date.strftime(src_format) for date in dates if date >= min_date]\n",
    "            return_min_date = (min_date + timedelta(days=1)).strftime(src_format)\n",
    "        else:\n",
    "            return_dates = []\n",
    "            return_min_date = datetime(2200, 1, 1).date()\n",
    "    except bucket.meta.client.exceptions.NoSuchKey:\n",
    "        return_dates = [(min_date + timedelta(days=x)).strftime(src_format) for x in range(0, (today-min_date).days + 1)]\n",
    "        return_min_date = arg_date\n",
    "    return return_min_date, return_dates\n",
    "\n",
    "def update_meta_file(bucket, meta_key, extract_date_list):\n",
    "    df_new = pd.DataFrame(columns=['source_date', 'datetime_of_processing'])\n",
    "    df_new['source_date'] = extract_date_list\n",
    "    df_new['datetime_of_processing'] = datetime.today().strftime('%Y-%m-%d')\n",
    "    df_old = read_csv_to_df(bucket, meta_key)\n",
    "    df_all = pd.concat([df_old, df_new])\n",
    "    write_df_to_s3_csv(bucket, df_all, meta_key)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b9d8be0c-fadd-463b-971c-fed94f42088d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function entrypoint\n",
    "\n",
    "def main():\n",
    "    # Parameters/Configurations\n",
    "    # Later read config\n",
    "    arg_date='2022-12-30'\n",
    "    src_format = '%Y-%m-%d'\n",
    "    src_bucket = 'xetra-1234'\n",
    "    trg_bucket = 'xetra-sm'\n",
    "    columns = ['ISIN', 'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice', 'EndPrice', 'TradedVolume']\n",
    "    trg_key = 'xetra_smitesh_report'\n",
    "    trg_format = '.parquet'\n",
    "    meta_key = 'meta_file.csv'\n",
    "    \n",
    "    # Init\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket_src = s3.Bucket(src_bucket)\n",
    "    bucket_trg = s3.Bucket(trg_bucket)\n",
    "    \n",
    "    # run application\n",
    "    extract_date, date_list = return_date_list(bucket_trg, arg_date, src_format, meta_key)\n",
    "    etl_report1(bucket_src, bucket_trg, date_list, columns, extract_date, trg_key, trg_format,meta_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e69590ee-e1a9-4160-8f1a-b38627850ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4716\\2579682883.py:7: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  src_dates = set(pd.to_datetime(df_meta['source_date']).dt.date)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_4716\\1485487011.py:5: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([read_csv_to_df(bucket, obj) for obj in files], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b243397e-3995-4513-be47-e28b85972cfa",
   "metadata": {},
   "source": [
    "## Reading the uploaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e88610e5-b75d-4f15-bca0-5dd5c24eba0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta_file.csv\n",
      "xetra_smitesh_report20240821_210632.parquet\n",
      "xetra_smitesh_report20240821_212312.parquet\n",
      "xetra_smitesh_report20240901_165708.parquet\n",
      "xetra_smitesh_report20240901_171909.parquet\n",
      "xetra_smitesh_report20240901_195045.parquet\n",
      "xetra_smitesh_report20240901_204213.parquet\n",
      "xetra_smitesh_report20240901_205024.parquet\n",
      "xetra_smitesh_report20240901_221323.parquet\n",
      "xetra_smitesh_report20240901_223928.parquet\n",
      "xetra_smitesh_report20240901_225542.parquet\n"
     ]
    }
   ],
   "source": [
    "trg_bucket = 'xetra-sm'\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_trg = s3.Bucket(trg_bucket)\n",
    "for obj in bucket_trg.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a790ba07-b8e6-4475-b048-936d0b9b00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prq_obj = bucket_trg.Object(key='xetra_smitesh_report20240901_225542.parquet').get().get('Body').read()\n",
    "data = BytesIO(prq_obj)\n",
    "df_report = pd.read_parquet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e147923f-0cea-43c2-a974-5a82dd506a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ISIN</th>\n",
       "      <th>Date</th>\n",
       "      <th>opening_price_eur</th>\n",
       "      <th>closing_price_eur</th>\n",
       "      <th>minimum_price_eur</th>\n",
       "      <th>maximum_price_eur</th>\n",
       "      <th>daily_traded_volume</th>\n",
       "      <th>change_prev_closing_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AT000000STR1</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>36.60</td>\n",
       "      <td>36.70</td>\n",
       "      <td>35.75</td>\n",
       "      <td>36.70</td>\n",
       "      <td>1773</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AT000000STR1</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>36.60</td>\n",
       "      <td>36.70</td>\n",
       "      <td>35.75</td>\n",
       "      <td>36.70</td>\n",
       "      <td>1773</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AT000000STR1</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>36.60</td>\n",
       "      <td>36.70</td>\n",
       "      <td>35.75</td>\n",
       "      <td>36.70</td>\n",
       "      <td>1773</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AT00000FACC2</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.57</td>\n",
       "      <td>7.87</td>\n",
       "      <td>8.57</td>\n",
       "      <td>10205</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AT00000FACC2</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>8.05</td>\n",
       "      <td>8.57</td>\n",
       "      <td>7.87</td>\n",
       "      <td>8.57</td>\n",
       "      <td>10205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9691</th>\n",
       "      <td>XS2376095068</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>34.29</td>\n",
       "      <td>36.50</td>\n",
       "      <td>34.06</td>\n",
       "      <td>36.50</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9692</th>\n",
       "      <td>XS2376095068</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>34.29</td>\n",
       "      <td>36.50</td>\n",
       "      <td>34.06</td>\n",
       "      <td>36.50</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9693</th>\n",
       "      <td>XS2434891219</td>\n",
       "      <td>2022-12-29</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9694</th>\n",
       "      <td>XS2434891219</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9695</th>\n",
       "      <td>XS2434891219</td>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>3.44</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.42</td>\n",
       "      <td>3.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9696 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ISIN        Date  opening_price_eur  closing_price_eur  \\\n",
       "0     AT000000STR1  2022-12-29              36.60              36.70   \n",
       "1     AT000000STR1  2022-12-30              36.60              36.70   \n",
       "2     AT000000STR1  2022-12-31              36.60              36.70   \n",
       "3     AT00000FACC2  2022-12-29               8.05               8.57   \n",
       "4     AT00000FACC2  2022-12-30               8.05               8.57   \n",
       "...            ...         ...                ...                ...   \n",
       "9691  XS2376095068  2022-12-30              34.29              36.50   \n",
       "9692  XS2376095068  2022-12-31              34.29              36.50   \n",
       "9693  XS2434891219  2022-12-29               3.44               3.66   \n",
       "9694  XS2434891219  2022-12-30               3.44               3.66   \n",
       "9695  XS2434891219  2022-12-31               3.44               3.66   \n",
       "\n",
       "      minimum_price_eur  maximum_price_eur  daily_traded_volume  \\\n",
       "0                 35.75              36.70                 1773   \n",
       "1                 35.75              36.70                 1773   \n",
       "2                 35.75              36.70                 1773   \n",
       "3                  7.87               8.57                10205   \n",
       "4                  7.87               8.57                10205   \n",
       "...                 ...                ...                  ...   \n",
       "9691              34.06              36.50                 1000   \n",
       "9692              34.06              36.50                 1000   \n",
       "9693               3.42               3.66                    0   \n",
       "9694               3.42               3.66                    0   \n",
       "9695               3.42               3.66                    0   \n",
       "\n",
       "      change_prev_closing_%  \n",
       "0                       NaN  \n",
       "1                       0.0  \n",
       "2                       0.0  \n",
       "3                       NaN  \n",
       "4                       0.0  \n",
       "...                     ...  \n",
       "9691                    0.0  \n",
       "9692                    0.0  \n",
       "9693                    NaN  \n",
       "9694                    0.0  \n",
       "9695                    0.0  \n",
       "\n",
       "[9696 rows x 8 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e7781-dfa4-4988-979f-6b546cc99e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
